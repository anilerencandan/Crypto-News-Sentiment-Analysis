{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasının yolu\n",
    "csv_dosya = '/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_news.csv'\n",
    "\n",
    "# CSV dosyasını oku\n",
    "df = pd.read_csv(csv_dosya)\n",
    "\n",
    "# Anahtar kelimeler listesi\n",
    "anahtar_kelime_listesi = ['BTC', 'bitcoin', 'ETH', 'etherium']\n",
    "\n",
    "# Satırlarda 'Başlık' veya 'İçerik' sütunlarında herhangi bir anahtar kelimenin geçip geçmediğini kontrol et\n",
    "df = df[\n",
    "    df['İçerik'].apply(lambda x: any(kelime in str(x).lower() for kelime in anahtar_kelime_listesi)) |\n",
    "    df['Başlık'].apply(lambda x: any(kelime in str(x).lower() for kelime in anahtar_kelime_listesi))\n",
    "]\n",
    "\n",
    "# Sonucu kontrol et\n",
    "df.info()\n",
    "\n",
    "df.head(50)\n",
    "output_path = '/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_news1.csv'\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "df = pd.read_csv('/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_news_tokenize.csv')\n",
    "\n",
    "# Coin terimlerini tanımla\n",
    "coin_terms = {'bitcoin', 'ethereum', 'solona', 'btc', 'eth', 'doge', 'sol', 'coin','openai', 'chatgpt4','chatgpt','chat','gpt'}\n",
    "\n",
    "\n",
    "def replace_punctuation(text):\n",
    "    # 1. İki kelime arasında bulunan \"-\" işaretini tek boşlukla değiştir\n",
    "    text = re.sub(r'(?<=\\w)-(?=\\w)', ' ', text)\n",
    "\n",
    "    # 2. Kelime içindeki \".\" işaretlerini koruyarak diğer tüm noktalama işaretlerini kaldır\n",
    "    text = re.sub(r\"(?<!\\w)\\.|(?<!\\d)\\.\", '', text)\n",
    "    text = re.sub(r\"[^\\w\\s.-]\", '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['Başlık_noktalamasız'] = df['Başlık_tokenized'].apply(lambda x: replace_punctuation(str(x)))\n",
    "df['İçerik_noktalamasız'] = df['İçerik_tokenized'].apply(lambda x: replace_punctuation(str(x)))\n",
    "\n",
    "df.head(50)\n",
    "df.to_csv('/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_newnoktalamasız.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Gerekli nltk veri setini indir\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Dosyayı yükle\n",
    "df = pd.read_csv('/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_newnoktalamasız.csv')\n",
    "\n",
    "# Özel stopwords listesi\n",
    "custom_stopwords = {'ın', 'in', 'ile', 'ler','lar', 'de', 'da', 'ki', 'bu', 'şu', 've', 'veya', 'ama', 'fakat', 'gibi', 'ise', 'değil', 'mı', 'mi', 'mu', 'mü', 'ne', 'ya', 'hem', 'ya da', 'yani','a','e','i','ı','o','ö','u','ü','daha','en','sonra','önce','şimdi','do','da','de','daha','en','sonra','önce','şimdi','do','da','de','daha','en','sonra','önce','şimdi','do','un','ün','bir','aradan','deki' ,'doku', 'daki', }\n",
    "\n",
    "# Stopwords temizleme fonksiyonu\n",
    "def remove_custom_stopwords(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in custom_stopwords]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "df['Başlık_noktalamasız1'] = df['Başlık_noktalamasız'].apply(lambda x: remove_custom_stopwords(str(x)) if isinstance(x, str) else x)\n",
    "df['İçerik_noktalamasız1'] = df['İçerik_noktalamasız'].apply(lambda x: remove_custom_stopwords(str(x)) if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "output_path = '/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_newnoktalamasız1.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Özel stopwords temizlenmiş veriler {output_path} dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv_dosya = '/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_newnoktalamasız1.csv'\n",
    "df = pd.read_csv(csv_dosya)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Tokenizasyonu Başlık ve İçerik sütunlarına uygula\n",
    "df['Başlık_tokenize1'] = df['Başlık_noktalamasız1'].apply(lambda x: tokenize_text(x) if isinstance(x, str) else x)\n",
    "df['İçerik_tokenize1'] = df['İçerik_noktalamasız1'].apply(lambda x: tokenize_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Sonucu kontrol et\n",
    "print(df[['Başlık', 'Başlık_tokenized', 'İçerik', 'İçerik_tokenized']].head(10))\n",
    "\n",
    "# Tokenize edilmiş veriyi yeni bir CSV dosyasına kaydet\n",
    "df.to_csv('/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_tokenize1.csv', index=False)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Tokenize edilmiş veriler {output_path} dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f706aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "df = pd.read_csv('/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_tokenize1.csv')\n",
    "\n",
    "stop_words = set(stopwords.words(\"turkish\"))\n",
    "\n",
    "\n",
    "korunan_kelime_listesi = {'bitcoin', 'solana', 'ethereum', 'btc', 'sol','eth','altcoin','openai','ai','open','doge','donald','trump','elon','musk','chat','gpt','chatgpt'}  # Bu kelimeleri stopwords'den hariç tutacağız\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return ' '.join([token for token in tokens if token not in stop_words or token in korunan_kelime_listesi])\n",
    "\n",
    "# Stopwords temizleme işlemini Başlık ve İçerik sütunlarına uygula\n",
    "df['Başlık_cleaned'] = df['Başlık_tokenize1'].apply(lambda x: remove_stopwords(x) if isinstance(x, str) else x)\n",
    "df['İçerik_cleaned'] = df['İçerik_tokenize1'].apply(lambda x: remove_stopwords(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Temizlenmiş sonuçları yeni bir CSV dosyasına kaydet\n",
    "output_path = '/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_news2.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Stopwords temizlenmiş veriler {output_path} dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "# CSV dosyasını okuma\n",
    "df = pd.read_csv(\"/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_news2.csv\")\n",
    "\n",
    "# TurkishStemmer nesnesi oluşturma\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "# Kök bulma işlemi\n",
    "df['Başlık_cleaned_son'] = df['Başlık_cleaned'].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
    "df['İçerik_cleaned_son'] = df['İçerik_cleaned'].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
    "\n",
    "\n",
    "# Sonuçları yeni bir CSV dosyasına kaydetme\n",
    "output_path = '/Users/anilcandan/Desktop/Yazlab-Scraping/csv_files/all_crypto_news3.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Stopwords temizlenmiş veriler {output_path} dosyasına kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasını yükleyin\n",
    "csv_dosya = '/Users/anilcandan/Desktop/all_crypto_news3.csv'\n",
    "df = pd.read_csv(csv_dosya)\n",
    "\n",
    "# Silmek istediğiniz sütunların isimlerini listeye ekleyin\n",
    "silinecek_sutunlar = ['Başlık', 'İçerik','Başlık_tokenized','İçerik_tokenized','Başlık_noktalamasız','İçerik_noktalamasız','İçerik_tokenized1','Başlık_tokenized1','Başlık_cleaned','İçerik_cleaned','Başlık_noktalamasız1','İçerik_noktalamasız1']\n",
    "\n",
    "# Belirtilen sütunları sil\n",
    "df = df.drop(silinecek_sutunlar, axis=1)\n",
    "\n",
    "# Sonucu kontrol edin\n",
    "print(df.head())\n",
    "\n",
    "# İsteğe bağlı: Değişiklikleri yeni bir CSV dosyasına kaydedin\n",
    "df.to_csv('/Users/anilcandan/Desktop/YazlabProje.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
